{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9763f7f",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058e93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.initializers import HeNormal, Constant\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from natsort import natsorted\n",
    "import datetime\n",
    "\n",
    "# Add matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d19b4e",
   "metadata": {},
   "source": [
    "## Configure GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd7181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU activation, if not 1 activate GPU using Edit -> Notebook Settings -> Hardware Accelerator -> GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs:\", len(physical_devices))\n",
    "# If Num GPUs >= 1 then you can continue with GPU enabled\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de012c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # COnfigure GPU to allow dynamic memory growth\n",
    "tf.device('/GPU:0')\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d95b0c",
   "metadata": {},
   "source": [
    "## Input Pipeline\n",
    "- Long term aim should be to replace ImageDataGenerator with much faster tf.data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e5d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set base data path for dataset\n",
    "data_path = 'D:/Complex/Rotation/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ae5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode function masks and depths \n",
    "def encode_one_hot(img, levels):\n",
    "    encoded = to_categorical(img, levels, dtype='uint8')\n",
    "    return np.asarray(encoded)\n",
    "\n",
    "\n",
    "# One hot decode pixel level\n",
    "def decode_one_hot(img):\n",
    "    decoded = tf.math.argmax(img, axis=2)\n",
    "    return np.asarray(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generator parameters\n",
    "frames_gen = ImageDataGenerator(rescale = 1/255.)\n",
    "masks_gen = ImageDataGenerator()\n",
    "zbuffs_gen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80da8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen(folder, batch_size, shuffle, seed = 5, target_size = (224,224)):\n",
    "\n",
    "    left_gen = frames_gen.flow_from_directory(data_path+folder+'frames/left/', seed = seed,\n",
    "                                              batch_size = batch_size, target_size = target_size, shuffle = shuffle)\n",
    "\n",
    "    right_gen = frames_gen.flow_from_directory(data_path+folder+'frames/right/', seed = seed,\n",
    "                                              batch_size = batch_size, target_size = target_size, shuffle = shuffle)\n",
    "\n",
    "    left_mask = masks_gen.flow_from_directory(data_path+folder+'masks/left/', seed = seed,\n",
    "                                              batch_size = batch_size, target_size = target_size, shuffle = shuffle,\n",
    "                                              color_mode = 'grayscale')\n",
    "    \n",
    "    right_mask = masks_gen.flow_from_directory(data_path+folder+'masks/right/', seed = seed,\n",
    "                                              batch_size = batch_size, target_size = target_size, shuffle = shuffle,\n",
    "                                              color_mode = 'grayscale')\n",
    "    \n",
    "    depth_gen = zbuffs_gen.flow_from_directory(data_path+folder+'zbuffs/left/', seed = seed,\n",
    "                                              batch_size = batch_size, target_size = target_size, shuffle = shuffle,\n",
    "                                              color_mode = 'grayscale')\n",
    "\n",
    "    while True:\n",
    "        # Get next image\n",
    "        left = left_gen.next()\n",
    "        right = right_gen.next()\n",
    "        left_target = left_mask.next()\n",
    "        right_target = right_mask.next()\n",
    "        depth = depth_gen.next()\n",
    "\n",
    "        # One hot encode\n",
    "        l_mask = [encode_one_hot(left_target[0][x,:,:,:], 256) for x in range(left_target[0].shape[0])]\n",
    "        r_mask = [encode_one_hot(right_target[0][x,:,:,:], 256) for x in range(right_target[0].shape[0])]\n",
    "        depth = [encode_one_hot(depth[0][x,:,:,:], 256) for x in range(depth[0].shape[0])]\n",
    "\n",
    "        yield [np.asarray(left[0]),np.asarray(right[0])], [np.asarray(l_mask),np.asarray(r_mask),np.asarray(depth)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6817fe0c",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3ab275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class for convolution block\n",
    "class ConvBlock(layers.Layer):\n",
    "    \"\"\" Defines convolution block: (Conv2D -> (Activation) -> BN) x2\n",
    "        Returns output of convolution block\n",
    "    \"\"\"\n",
    "    def __init__(self, num_filters):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        # Conv -> (Activation) -> BN\n",
    "        self.conv1 = layers.Conv2D(num_filters, kernel_size = (3, 3), activation = 'elu',\n",
    "                                  kernel_initializer = tf.keras.initializers.HeNormal(),\n",
    "                                  use_bias = True, bias_initializer = tf.keras.initializers.Constant(0.1),\n",
    "                                  padding = 'same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        # Conv -> (Activation) -> BN\n",
    "        self.conv2 = layers.Conv2D(num_filters, kernel_size = (3, 3), activation = 'elu',\n",
    "                                  kernel_initializer = tf.keras.initializers.HeNormal(),\n",
    "                                  use_bias = True, bias_initializer = tf.keras.initializers.Constant(0.1),\n",
    "                                  padding = 'same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "    # Create object\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        return x\n",
    "\n",
    "    # Define config for custom layer\n",
    "    def get_config(self):\n",
    "        config = super(ConvBlock, self).get_config()\n",
    "        config.update({'cb_conv1': self.conv1, 'cb_bn1': self.bn1, 'cb_conv2': self.conv2, 'cb_bn2': self.bn2})\n",
    "        return config\n",
    "    \n",
    "    \n",
    "# Define class for encoder layer\n",
    "class EncoderLayer(layers.Layer):\n",
    "    \"\"\" Defines encoder layer: ConvBlock -> MaxPool2D -> SpatialDropout2D\n",
    "        Returns layer and pooling output for next downward layer input and skip connection\n",
    "    \"\"\"\n",
    "    def __init__(self, num_filters, dropout=0.):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.conv_block = ConvBlock(num_filters)\n",
    "        self.max_pool = layers.MaxPooling2D((2, 2))\n",
    "        self.dropout = layers.SpatialDropout2D(dropout)\n",
    "    \n",
    "    # Create object\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv_block(input_tensor, training=training)\n",
    "        p = self.max_pool(x)\n",
    "        p = self.dropout(p)\n",
    "        return x, p\n",
    "\n",
    "    # Define config for custom layer\n",
    "    def get_config(self):\n",
    "        config = super(EncoderLayer, self).get_config()\n",
    "        config.update({'el_conv_block': self.conv_block, 'el_max_pool': self.max_pool, 'el_dropout_2d': self.dropout})\n",
    "        return config\n",
    "    \n",
    "\n",
    "# Define class for decoder layer\n",
    "class DecoderLayer(layers.Layer):\n",
    "    \"\"\" Defines Decoder layer: UpSample2D -> Concatenate -> ConvBlock\n",
    "        Returns layer output for next upward layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_filters):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.upsample = layers.UpSampling2D((2, 2))\n",
    "        self.concat = layers.Concatenate()\n",
    "        self.conv_block = ConvBlock(num_filters)\n",
    "    \n",
    "    # Create object\n",
    "    def call(self, input_tensor, skip_conn, training=False):\n",
    "        x = self.upsample(input_tensor)\n",
    "        x = self.concat([x, skip_conn])\n",
    "        x = self.conv_block(x, training=training)\n",
    "        return x\n",
    "\n",
    "    # Define config for custom layer\n",
    "    def get_config(self):\n",
    "        config = super(DecoderLayer, self).get_config()\n",
    "        config.update({'dl_upsample': self.upsample, 'dl_concat': self.concat, 'dl_conv_block': self.conv_block})\n",
    "        return config\n",
    "\n",
    "\n",
    "# Define class for upsampling with skip connections\n",
    "class UpsampleSkipConn(layers.Layer):\n",
    "    \"\"\" Defines upsample for merged inputs with skip connections for (224, 224) image\n",
    "        Returns a merged depth input for second half of double u-net\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(UpsampleSkipConn, self).__init__()\n",
    "        self.upsample = layers.UpSampling2D((2, 2))\n",
    "        self.concat = layers.Concatenate()\n",
    "    \n",
    "    # Create object\n",
    "    def call(self, input_tensor, skip_conn, training=False):\n",
    "        x = self.upsample(input_tensor)\n",
    "        x = self.concat([x, skip_conn[0], skip_conn[1]])\n",
    "        return x\n",
    "    \n",
    "    # Define config for custom layer\n",
    "    def get_config(self):\n",
    "        config = super(UpSampleSkipConn, self).get_config()\n",
    "        config.update({'usc_upsample': self.upsample, 'usc_concat': self.concat})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd60e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StereoDepthNet(Model):\n",
    "    \"\"\" Build double U-Net for segmentation and depth estimation \"\"\"\n",
    "    def __init__(self, seg_channels = 256, depth_channels = 256):\n",
    "        super(StereoDepthNet, self).__init__()\n",
    "        self.filters = [16, 32, 64, 128, 256]\n",
    "        self.softmax_kernel = (1, 1)\n",
    "        self.activate = 'softmax'\n",
    "        self.padding = 'same'\n",
    "\n",
    "        # Frame encoder block layers with downscale pooling\n",
    "        # Input H&W: 224 -> 112 -> 56 -> 28 -> 14 Bridge layer\n",
    "        self.left_frame1 = EncoderLayer(self.filters[0], dropout=0.1)\n",
    "        self.left_frame2 = EncoderLayer(self.filters[1], dropout=0.1)\n",
    "        self.left_frame3 = EncoderLayer(self.filters[2], dropout=0.2)\n",
    "        self.left_frame4 = EncoderLayer(self.filters[3], dropout=0.3)\n",
    "\n",
    "        self.right_frame1 = EncoderLayer(self.filters[0], dropout=0.1)\n",
    "        self.right_frame2 = EncoderLayer(self.filters[1], dropout=0.1)\n",
    "        self.right_frame3 = EncoderLayer(self.filters[2], dropout=0.2)\n",
    "        self.right_frame4 = EncoderLayer(self.filters[3], dropout=0.3)\n",
    "\n",
    "        # Bridge layer\n",
    "        self.left_bridge_in = ConvBlock(self.filters[4])\n",
    "        self.right_bridge_in = ConvBlock(self.filters[4])\n",
    "        self.merged = layers.Concatenate()\n",
    "        self.left_bridge_out = ConvBlock(self.filters[4])\n",
    "        self.right_bridge_out = ConvBlock(self.filters[4])\n",
    "\n",
    "        # Segmentation decoder block layers with upsampling & skip connections\n",
    "        self.left_seg4 = DecoderLayer(self.filters[3])\n",
    "        self.left_seg3 = DecoderLayer(self.filters[2])\n",
    "        self.left_seg2 = DecoderLayer(self.filters[1])\n",
    "        self.left_seg1 = DecoderLayer(self.filters[0])\n",
    "\n",
    "        self.right_seg4 = DecoderLayer(self.filters[3])\n",
    "        self.right_seg3 = DecoderLayer(self.filters[2])\n",
    "        self.right_seg2 = DecoderLayer(self.filters[1])\n",
    "        self.right_seg1 = DecoderLayer(self.filters[0])\n",
    "\n",
    "        # Segmentation mask outputs\n",
    "        self.left_mask = layers.Conv2D(seg_channels, self.softmax_kernel, activation = self.activate, padding = self.padding)\n",
    "        self.right_mask = layers.Conv2D(seg_channels, self.softmax_kernel, activation = self.activate, padding = self.padding)\n",
    "\n",
    "        \n",
    "        # Upsample self.merged to full size (224) to form a depth input\n",
    "        self.upsample_merge4 = UpsampleSkipConn()\n",
    "        self.upsample_merge3 = UpsampleSkipConn()\n",
    "        self.upsample_merge2 = UpsampleSkipConn()\n",
    "        self.upsample_merge1 = UpsampleSkipConn()\n",
    "\n",
    "\n",
    "        # Depth encoder block layers with downscale pooling\n",
    "        # Input H&W: 224 -> 112 -> 56 -> 28 -> 14 Bridge layer\n",
    "        self.depth_in1 = EncoderLayer(self.filters[0], dropout=0.1)\n",
    "        self.depth_in2 = EncoderLayer(self.filters[1], dropout=0.1)\n",
    "        self.depth_in3 = EncoderLayer(self.filters[2], dropout=0.2)\n",
    "        self.depth_in4 = EncoderLayer(self.filters[3], dropout=0.3)\n",
    "\n",
    "        # Depth bridge\n",
    "        self.depth_bridge_in = ConvBlock(self.filters[4])\n",
    "        self.depth_bridge1 = ConvBlock(self.filters[4])\n",
    "        self.depth_bridge2 = ConvBlock(self.filters[4])\n",
    "        self.depth_bridge3 = ConvBlock(self.filters[4])\n",
    "        self.depth_bridge_out = ConvBlock(self.filters[4])\n",
    "\n",
    "        # Depth decoder block layers with upsampling & skip connections\n",
    "        self.depth_out4 = DecoderLayer(self.filters[3])\n",
    "        self.depth_out3 = DecoderLayer(self.filters[2])\n",
    "        self.depth_out2 = DecoderLayer(self.filters[1])\n",
    "        self.depth_out1 = DecoderLayer(self.filters[0])\n",
    "\n",
    "        # Depth estimate output\n",
    "        self.depth = layers.Conv2D(depth_channels, self.softmax_kernel, activation = self.activate, padding = self.padding)\n",
    "\n",
    "        \n",
    "    # Create object\n",
    "    def call(self, input_tensor, training=False):\n",
    "        # Segmentation encoder block layers with skip connection, pooling and spatial dropout\n",
    "        skl1, xl1 = self.left_frame1(input_tensor[0], training=training)\n",
    "        skl2, xl2 = self.left_frame2(xl1, training=training)\n",
    "        skl3, xl3 = self.left_frame3(xl2, training=training)\n",
    "        skl4, xl4 = self.left_frame4(xl3, training=training)\n",
    "        \n",
    "        skr1, xr1 = self.right_frame1(input_tensor[1], training=training)\n",
    "        skr2, xr2 = self.right_frame2(xr1, training=training)\n",
    "        skr3, xr3 = self.right_frame3(xr2, training=training)\n",
    "        skr4, xr4 = self.right_frame4(xr3, training=training)\n",
    "        \n",
    "        # Bridge layer\n",
    "        xl5 = self.left_bridge_in(xl4, training=training)\n",
    "        xr5 = self.right_bridge_in(xr4, training=training)\n",
    "        merged = self.merged([xl5, xr5])\n",
    "        left_bridge_out = self.left_bridge_out(merged, training=training)\n",
    "        right_bridge_out = self.right_bridge_out(merged, training=training)\n",
    "\n",
    "        # Segmentation decoder block layers\n",
    "        ls4 = self.left_seg4(left_bridge_out, skl4, training=training)\n",
    "        ls3 = self.left_seg3(ls4, skl3, training=training)\n",
    "        ls2 = self.left_seg2(ls3, skl2, training=training)\n",
    "        ls1 = self.left_seg1(ls2, skl1, training=training)\n",
    "        # Left segmentation output defined at end of function\n",
    "\n",
    "        rs4 = self.right_seg4(right_bridge_out, skr4, training=training)\n",
    "        rs3 = self.right_seg3(rs4, skr3, training=training)\n",
    "        rs2 = self.right_seg2(rs3, skr2, training=training)\n",
    "        rs1 = self.right_seg1(rs2, skr1, training=training)\n",
    "        # Right segmentation output defined at end of function\n",
    "        \n",
    "        # Upsample merged and add skip connections for merged depth input (224, 224)\n",
    "        merge4 = self.upsample_merge4(merged, [skl4, skr4], training=training)\n",
    "        merge3 = self.upsample_merge3(merge4, [skl3, skr3], training=training)\n",
    "        merge2 = self.upsample_merge2(merge3, [skl2, skr2], training=training)\n",
    "        depth_input = self.upsample_merge1(merge2, [skl1, skr1], training=training)\n",
    "        \n",
    "        # Depth encoder block layers with skip connection, pooling and spatial dropout\n",
    "        skd1, xd1 = self.depth_in1(depth_input, training=training)\n",
    "        skd2, xd2 = self.depth_in2(xd1, training=training)\n",
    "        skd3, xd3 = self.depth_in3(xd2, training=training)\n",
    "        skd4, xd4 = self.depth_in4(xd3, training=training)\n",
    "\n",
    "        # Depth bridge\n",
    "        xd5 = self.depth_bridge_in(xd4, training=training)\n",
    "        bx1 = self.depth_bridge1(xd5, training=training)\n",
    "        bx2 = self.depth_bridge2(bx1, training=training)\n",
    "        bx3 = self.depth_bridge3(bx2, training=training)\n",
    "        de5 = self.depth_bridge_out(bx3, training=training)\n",
    "\n",
    "        # Depth decoder block layers with upsampling and skip connection\n",
    "        de4 = self.depth_out4(de5, skd4, training=training)\n",
    "        de3 = self.depth_out3(de4, skd3, training=training)\n",
    "        de2 = self.depth_out2(de3, skd2, training=training)\n",
    "        de1 = self.depth_out1(de2, skd1, training=training)\n",
    "        # Depth estimation output defined at end of function\n",
    "\n",
    "        # Outputs\n",
    "        left_mask = self.left_mask(ls1)\n",
    "        right_mask = self.right_mask(rs1)\n",
    "        depth = self.depth(de1)\n",
    "        return [left_mask, right_mask, depth]\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\"hidden_units\": self.hidden_units}\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13712bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = StereoDepthNet(seg_channels=256, depth_channels=256)\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = ['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'],\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528ab755",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ab7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "early_stop = 40\n",
    "experiment = '_Summary_ComplexRotation_Updated_Depth'\n",
    "best_model = experiment + '_StereoDepthNetv2_' + datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "mc = ModelCheckpoint(mode='min', filepath=('Models/'+best_model+'.h5'), monitor='val_loss',\n",
    "                     save_best_only='True', save_weights_only='True', verbose=2)\n",
    "es = EarlyStopping(mode='min', monitor='val_loss', patience=early_stop, verbose=2)\n",
    "#tb = TensorBoard(log_dir='TensorBoard/SDNv2//{}'.format(best_model), write_graph=True, histogram_freq=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=2, mode='auto', min_delta=0.00001,\n",
    "                              cooldown=0, min_lr=0)\n",
    "\n",
    "# callbacks = [tb, mc, es, reduce_lr]\n",
    "callbacks = [mc, es, reduce_lr]\n",
    "smooth = 1\n",
    "\n",
    "# calculate step sizes from batch size of length of training and validation data\n",
    "num_epochs = 200\n",
    "batch_size = 4\n",
    "train_steps_path = data_path + 'training/frames/left/left/'\n",
    "val_steps_path = data_path + 'validation/frames/left/left/'\n",
    "\n",
    "train_size = len(os.listdir(train_steps_path))\n",
    "val_size = len(os.listdir(val_steps_path))\n",
    "\n",
    "train_steps_epoch = int(np.ceil(train_size / batch_size))\n",
    "val_steps_epoch = int(np.ceil(val_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23922fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "DepthResult = model.fit(datagen(folder='training/', batch_size=batch_size, shuffle=True),\n",
    "                        validation_data = datagen(folder='validation/', batch_size=batch_size, shuffle=False),\n",
    "                        steps_per_epoch = train_steps_epoch, validation_steps = val_steps_epoch,\n",
    "                        epochs = num_epochs, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d6e700",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "Epoch 189/200\n",
    "900/900 [==============================] - ETA: 0s - loss: 0.4883 - output_1_loss: 0.0495 - output_2_loss: 0.0501 - output_3_loss: 0.3888 - output_1_accuracy: 0.9786 - output_2_accuracy: 0.9786 - output_3_accuracy: 0.8727\n",
    "Epoch 189: val_loss improved from 0.57406 to 0.57254, saving model to Models\\_Summary_ComplexRotation_Updated_Depth_StereoDepthNetv2_20220324_201552.h5\n",
    "900/900 [==============================] - 183s 203ms/step - loss: 0.4883 - output_1_loss: 0.0495 - output_2_loss: 0.0501 - output_3_loss: 0.3888 - output_1_accuracy: 0.9786 - output_2_accuracy: 0.9786 - output_3_accuracy: 0.8727 - val_loss: 0.5725 - val_output_1_loss: 0.0803 - val_output_2_loss: 0.0767 - val_output_3_loss: 0.4155 - val_output_1_accuracy: 0.9719 - val_output_2_accuracy: 0.9716 - val_output_3_accuracy: 0.8667 - lr: 2.5000e-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e13d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use to initialise model until fix model.save\n",
    "# DepthResult = model.fit(datagen(folder='training/', batch_size=batch_size, shuffle=True),\n",
    "#                         validation_data = datagen(folder='validation/', batch_size=batch_size, shuffle=False),\n",
    "#                         steps_per_epoch = train_steps_epoch, validation_steps = val_steps_epoch,\n",
    "#                         epochs = num_epochs, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb3a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary - layer names, no. of parameters, weights etc.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d454645",
   "metadata": {},
   "source": [
    "## Training Accuracy (TensorBoard removed for training speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eff6ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot\n",
    "plt.plot(DepthResult.history['loss'], label='loss')\n",
    "plt.plot(DepthResult.history['val_loss'], label='val_loss')\n",
    "plt.ylim([0, 4])\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8198547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth accuracy plot\n",
    "plt.plot(DepthResult.history['output_3_accuracy'], label='depth_accuracy')\n",
    "plt.plot(DepthResult.history['val_output_3_accuracy'], label='val_depth_accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad73a40",
   "metadata": {},
   "source": [
    "## Save and Load Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b38dd68",
   "metadata": {},
   "source": [
    "#### Reload best training model and save full model execution graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model from training\n",
    "model_name = 'redacted'\n",
    "# Use model.save (for custom layers), ignore warnings\n",
    "model.save(model_name, save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7889c376",
   "metadata": {},
   "source": [
    "#### Load full model for standalone predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4dfead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model with compile=False as not training\n",
    "model = tf.keras.models.load_model(\"sdnv2\", compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6fef88",
   "metadata": {},
   "source": [
    "#### Load best weights if continuing on from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0778f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights to test\n",
    "model.load_weights('Models/'+best_model+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dbf57a",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "Check data_path at start of Input Pipeline for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79283aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpacks predictions upfront due to excessive memory consumption on large datasets\n",
    "batch_size = 4\n",
    "\n",
    "# Prediction generator\n",
    "testing_gen = datagen(folder='testing/', batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Calculate test steps\n",
    "test_steps_path = data_path + 'testing/frames/left/left/'\n",
    "test_size = len(os.listdir(test_steps_path))\n",
    "test_steps = int(np.ceil(test_size / batch_size))\n",
    "count = 0\n",
    "\n",
    "# Create empty lists for truths and predictions\n",
    "true_left_frames, true_left_masks, true_depths = [], [], []\n",
    "pred_left_masks, pred_right_masks, pred_depths = [], [], []\n",
    "\n",
    "# Start prediction loop\n",
    "start = datetime.datetime.now()\n",
    "while count < test_steps:\n",
    "\n",
    "    # Get next batch of data from data generator\n",
    "    [left_imgs, right_imgs], [true_left_mask, true_right_mask, true_depth] = next(testing_gen)\n",
    "    \n",
    "    # De-batch relevant input data (TRY block for end of (incomplete) batch preds without error)\n",
    "    for i in range(batch_size):\n",
    "        try:\n",
    "            true_left_frames.append(left_imgs[i])\n",
    "            true_left_masks.append(decode_one_hot(true_left_mask[i]))\n",
    "            true_depths.append(decode_one_hot(true_depth[i]))\n",
    "        except:\n",
    "            print(f\"Error de-batching on batch {count+1} at batch index {i}\")\n",
    "\n",
    "\n",
    "    # Make predictions (preds[left, right, depth] = [3, (batch_size, 224, 224, 256)])\n",
    "    preds = model.predict([left_imgs, right_imgs])\n",
    "\n",
    "    # De-batch prediction outputs (right mask not used in analysis)\n",
    "    for i in range(batch_size):\n",
    "        try:\n",
    "            pred_left_masks.append(decode_one_hot(preds[0][i]))\n",
    "            pred_depths.append(decode_one_hot(preds[2][i]))\n",
    "        except:\n",
    "            print(f\"Error de-batching on batch {count+1} at batch index {i}\")\n",
    "        \n",
    "    # Increment count\n",
    "    count += 1\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"\\nPredictions complete, time taken: \", end-start)\n",
    "print(f\"Input data lengths: true left frames {len(true_left_frames)}\")\n",
    "print(f\"Input data lengths: true left masks {len(true_left_masks)}; true depths {len(true_depths)}\")\n",
    "print(f\"Prediction lengths: pred left masks {len(pred_left_masks)}; pred depths {len(pred_depths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212346e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create depth errors list (256 depth levels)\n",
    "depth_errors = []\n",
    "for i in range(len(true_depths)):\n",
    "    depth_errors.append(np.asarray(true_depths[i]-pred_depths[i]))\n",
    "\n",
    "# Calculate histogram of errors list\n",
    "hist_all = []\n",
    "for i in range(len(depth_errors)):\n",
    "    hist, _ = np.histogram(depth_errors[i], bins=511, range=(-255, 255))\n",
    "    hist_all.append(hist)\n",
    "\n",
    "# Exact hit accuracy (h*w = 224*224 = 50176)\n",
    "test_acc = [hist_all[i][255]/50176 for i in range(len(hist_all))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7b4411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save csv of histogram of errors\n",
    "err_dist = list(range(-255, 256))\n",
    "with open(\"D:/RawPixels/ComplexRotation Updated Depth/histogram_of_errors.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(err_dist)\n",
    "    writer.writerows(hist_all)\n",
    "    \n",
    "# create and saves csv of test accuracies (from histogram of errors)\n",
    "# test_acc = [hist_all[i][255]/50176 for i in range(len(hist_all))]\n",
    "with open(\"D:/RawPixels/ComplexRotation Updated Depth/test_accuracy.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443c0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show overall test accuracy\n",
    "print(f\"Min accuracy:\\t{round(np.min(test_acc)*100, 1)}%\")\n",
    "print(f\"Max accuracy:\\t{round(np.max(test_acc)*100, 1)}%\")\n",
    "print(f\"Mean accuracy:\\t{round(np.mean(test_acc)*100, 1)}%\\n\")\n",
    "\n",
    "chart_acc = [test_acc[i]*100 for i in range(len(test_acc))]\n",
    "# Plot accuracy for each image\n",
    "plt.title(\"Accuracy across Test Set\\n\")\n",
    "plt.xlabel(\"Test Set Image (idx)\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.ylim([0, 100])\n",
    "plt.plot(chart_acc)\n",
    "plt.show()\n",
    "print()\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "# don't show outlier points\n",
    "plt.boxplot(chart_acc)\n",
    "plt.title(\"Test accuracy boxplot with outliers\")\n",
    "plt.xticks(color='w')\n",
    "plt.xlabel(\"Test Set: 1000 images\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1fade5",
   "metadata": {},
   "source": [
    "## Visualise Predictions\n",
    "Visualise results bank for user defined range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99887579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise predictions\n",
    "start_idx = 1\n",
    "end_idx = start_idx + 4\n",
    "\n",
    "for i in range(start_idx, end_idx):\n",
    "    \n",
    "    fig = plt.figure(figsize=(35,12))\n",
    "\n",
    "    ax1 = fig.add_subplot(1,6,1)\n",
    "    ax1.imshow(true_left_frames[i])\n",
    "    ax1.title.set_text('Actual Left Frame')\n",
    "    ax1.grid(False)\n",
    "\n",
    "    ax2 = fig.add_subplot(1,6,2)\n",
    "    ax2.set_title('Ground Truth Labels')\n",
    "    ax2.imshow(true_left_masks[i])\n",
    "    ax2.grid(False)\n",
    "\n",
    "    ax3 = fig.add_subplot(1,6,3)\n",
    "    ax3.set_title('Predicted Labels')\n",
    "    ax3.imshow(pred_left_masks[i])\n",
    "    ax3.grid(False)\n",
    "\n",
    "    ax4 = fig.add_subplot(1,6,4)\n",
    "    ax4.set_title('Ground Truth Depth')\n",
    "    ax4.grid(False)\n",
    "    ax4= plt.imshow(true_depths[i], cmap='gray')\n",
    "\n",
    "    ax5 = fig.add_subplot(1,6,5)\n",
    "    ax5.set_title('Predicted Depth')\n",
    "    ax5.grid(False)\n",
    "    ax5 = plt.imshow(pred_depths[i], cmap='gray')\n",
    "\n",
    "    ax6 = fig.add_subplot(1,6,6)\n",
    "    ax6.set_title('Depth Errors')\n",
    "    ax6 = plt.imshow(abs(depth_errors[i]), cmap='gray')\n",
    "\n",
    "#plt.saveplot('StereoSegResultsRight.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa3e9b2",
   "metadata": {},
   "source": [
    "## Individual Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b06adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best and worst images\n",
    "print(f\"Index of max accuracy: {test_acc.index(np.max(test_acc))}\\tAccuracy: {round(np.max(test_acc)*100, 1)}%\")\n",
    "print(f\"Index of min accuracy: {test_acc.index(np.min(test_acc))}\\tAccuracy: {round(np.min(test_acc)*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1fdf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select image index from test set (starts at zero) - Summary documents image number = idx+1 (position in dataset)\n",
    "image_idx = 86\n",
    "\n",
    "print(f\"\\nImage accuracy: {round(test_acc[image_idx]*100, 1)}%\\n\")\n",
    "\n",
    "# Configure and draw the histogram figure\n",
    "histogram, bin_edges = np.histogram(depth_errors[image_idx], bins=511, range=(-255, 255))\n",
    "plt.figure()\n",
    "plt.title(\"Grayscale Histogram (50,176 Pixels Predictions)\")\n",
    "plt.xlabel(\"Grayscale Error\")\n",
    "plt.ylabel(\"Pixel Count\")\n",
    "plt.xlim([-255, 255])  # <- named arguments do not work here\n",
    "plt.plot(bin_edges[0:-1], histogram)  # <- or here\n",
    "plt.show()\n",
    "print()\n",
    "# Zoom in on y-axis to see distribution of errors\n",
    "plt.figure()\n",
    "plt.title(\"ZOOM: Grayscale Histogram (50,176 Pixels Predictions)\")\n",
    "plt.xlabel(\"Grayscale Error\")\n",
    "plt.ylabel(\"Pixel Count\")\n",
    "plt.xlim([-255, 255])  # <- named arguments do not work here\n",
    "plt.ylim([0, 100])  # <- named arguments do not work here\n",
    "plt.plot(bin_edges[0:-1], histogram)  # <- or here\n",
    "plt.show()\n",
    "print()\n",
    "\n",
    "# Show larger depth images\n",
    "fig = plt.figure(figsize=(35,12))\n",
    "ax1 = fig.add_subplot(1,3,1)\n",
    "ax1 = plt.title(\"Actual Left Frame\\n\", fontsize=24)\n",
    "ax1 = plt.imshow(true_left_frames[image_idx])\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax2 = plt.title(\"Ground Truth Labels\\n\", fontsize=24)\n",
    "ax2 = plt.imshow(true_left_masks[image_idx])\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "ax3 = plt.title(\"Predicted Labels\\n\", fontsize=24)\n",
    "ax3 = plt.imshow(pred_left_masks[image_idx])\n",
    "\n",
    "fig = plt.figure(figsize=(35,12))\n",
    "ax1 = fig.add_subplot(1,3,1)\n",
    "ax1 = plt.title(\"Ground Truth Depth\\n\", fontsize=24)\n",
    "ax1 = plt.imshow(true_depths[image_idx], cmap='gray')\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax2 = plt.title(\"Predicted Depth\\n\", fontsize=24)\n",
    "ax2 = plt.imshow(pred_depths[image_idx], cmap='gray')\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "ax3 = plt.title(\"Depth Errors\\n\", fontsize=24)\n",
    "ax3 = plt.imshow(abs(depth_errors[image_idx]), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9a3ffa",
   "metadata": {},
   "source": [
    "## Individual Image Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12bc9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for matching of unique of pixels values\n",
    "true_unique = set()\n",
    "pred_unique = set()\n",
    "for image_idx in range(len(true_left_masks)):\n",
    "    a = list(np.unique(true_left_masks[image_idx]))\n",
    "    true_unique.update(a)\n",
    "    b = list(np.unique(pred_left_masks[image_idx]))\n",
    "    pred_unique.update(b)\n",
    "\n",
    "# Set of unique pixels across all images should match\n",
    "print(natsorted(true_unique))\n",
    "print(natsorted(pred_unique))\n",
    "\n",
    "# Convert to list for future use\n",
    "true_unique = natsorted(tuple(true_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09e9f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0, 63, 76, 88, 97, 99, 127, 132, 150, 155, 156, 166, 176, 215, 255]\n",
    "# labels = ['Background', 'FireSupport1Prefab', 'Barrel1Prefab', 'BoxPrefab', 'TowerPrefab', 'SandBarricadePrefab', 'CanopyPrefab', 'HangerPrefab', 'FuelTankPrefab', 'Car5Prefab', 'HighWallPrefab', 'Terrain', 'Foliage', 'CabinPrefab']\n",
    "labels = ['list of redacted labels']\n",
    "seg_labels = ['list of redacted labels']\n",
    "\n",
    "# Extract unique colours using same index as previous section\n",
    "a = list(np.unique(true_left_masks[image_idx]))\n",
    "# Extract unique colours\n",
    "b = list(np.unique(pred_left_masks[image_idx]))\n",
    "print(\"Unique true greyscale values:\\t\\t\", a)\n",
    "print(\"Unique predicted greyscale values:\\t\", b)\n",
    "print()\n",
    "\n",
    "# Get unique values, frequnecy count & first index position\n",
    "unique_true, occurCount= np.unique(true_left_masks[image_idx], return_counts=True)\n",
    "true_masks = tuple(zip(unique_true, occurCount))\n",
    "print(\"True left mask breakdown\")\n",
    "for elem in true_masks:\n",
    "    print(f\"Value  {elem[0]}:\\t{elem[1]} pixels\\tProportion of image: {round(elem[1]/50176*100, 1)}%\")\n",
    "print()\n",
    "\n",
    "# Get unique values, frequnecy count & first index position\n",
    "unique_pred, occurCount= np.unique(pred_left_masks[image_idx], return_counts=True)\n",
    "pred_masks = tuple(zip(unique_pred, occurCount))\n",
    "print(\"Predicted left mask breakdown\")\n",
    "for elem in pred_masks:\n",
    "    print(f\"Value  {elem[0]}:\\tProportion of image: {round(elem[1]/50176*100, 1)}%\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f1f41b",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eaa74c",
   "metadata": {},
   "source": [
    "#### Depth Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba989fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty confusion matrix\n",
    "cm_depth = np.zeros((256, 256), dtype='int')\n",
    "\n",
    "# Verify from test_accuracy calculations\n",
    "print(\"Calculating confusion matrix. Please wait...\\n\")\n",
    "for img in range(len(true_depths)):\n",
    "    for x in range(224):\n",
    "        for y in range(224):\n",
    "            true = true_depths[img][x][y]\n",
    "            pred = pred_depths[img][x][y]\n",
    "            cm_depth[pred, true] += 1\n",
    "#             if true==pred:\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 cm_depth[pred, true] += 1\n",
    "            \n",
    "print(\"Total pixels:\", np.sum(cm_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad2de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcs and print summary\n",
    "tot_pix = len(true_depths*224*224)\n",
    "print(f\"Total number of pixels in non-direct hits:\\t{np.sum(cm_depth)}\")\n",
    "print(f\"Total number of pixels in dataset:\\t\\t{tot_pix}\")\n",
    "print(f\"Percentage of non-direct hits:\\t\\t\\t{round(np.sum(cm_depth)/tot_pix*100, 1)}%\")\n",
    "print(\"\\nColour bar represents percentage of non-direct hits from within the error population.\")\n",
    "print(\"Thin red line represents axis of direct hits.\")\n",
    "\n",
    "# Convert numbrs to percentage\n",
    "cm2 = cm_depth / np.sum(cm_depth) *100\n",
    "\n",
    "# Plot error matrix\n",
    "plt.figure(figsize=(35,12))\n",
    "plt.title(\"Monocular Complex Rotation - Depth Confusion Matrix Across All Test Images (0-200m)\\n\", fontsize=14)\n",
    "plt.xlabel(\"Predicted\", fontsize=12)\n",
    "plt.ylabel(\"True\", fontsize=12)\n",
    "plt.xlim([-5, 260])\n",
    "plt.ylim([260, -5])\n",
    "plt.plot(range(200), color='red', linestyle = 'dotted', lw=0.4)\n",
    "plt.imshow(cm2, cmap='binary')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a314cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion matrix as table ready for export to csv\n",
    "labels=list(range(0, 256))\n",
    "df = pd.DataFrame(cm_depth, columns=labels, index=labels)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed64f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "df.to_csv(\"D:/ConfusionMatrix/_Summary - Depth Error ComplexRotation Confusion Matrix.csv\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854421aa",
   "metadata": {},
   "source": [
    "#### Segmentation Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a253a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title=f'\\nSegmentation Confusion Matrix ({len(true_depths)} images)\\n',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('\\nConfusion matrix, without normalization')\n",
    "\n",
    "    plt.figure(figsize=(35,12))\n",
    "    # Display as percentage\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "               horizontalalignment=\"center\",\n",
    "               color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    #   plt.tight_layout()\n",
    "    # configure and draw the histogram figure\n",
    "    plt.ylabel(\"\\nTrue label\")\n",
    "    plt.xlabel(\"\\nPredicted label\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e61099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty confusion matrix\n",
    "cm_seg = np.zeros((len(true_unique), len(true_unique)), dtype='int')\n",
    "\n",
    "# Verify from test_accuracy calculations\n",
    "print(\"Calculating confusion matrix. Please wait...\\n\")\n",
    "for img in range(len(true_left_masks)):\n",
    "    for x in range(224):\n",
    "        for y in range(224):\n",
    "            true = true_left_masks[img][x][y]\n",
    "            pred = pred_left_masks[img][x][y]\n",
    "            cm_seg[true_unique.index(pred), true_unique.index(true)] += 1\n",
    "print(\"Total pixels:\", np.sum(cm_seg))\n",
    "\n",
    "# Plot confusion matrix\n",
    "title=f'\\nSegmentation Confusion Matrix ({len(true_depths)} images)\\n'\n",
    "plot_confusion_matrix(cm_seg, seg_labels, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a4a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results table\n",
    "mydata = [(true_unique[i], cm[i][i], np.sum(cm[i]), round(cm[i][i]/np.sum(cm[i])*100, 2)) for i in range(len(true_unique))]\n",
    "head = [\"Pixel Value\", \"Predicted\", \"Total\", \"Accuracy\"]\n",
    "df = pd.DataFrame(mydata, columns=head, index=seg_labels)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1474627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_pred_acc = [round(cm[i][i]/np.sum(cm[i])*100, 2) for i in range(len(true_unique))]\n",
    "# Plot accuracy for aeach image\n",
    "plt.title(\"Segmentation class accuracy across test set\\n\")\n",
    "plt.xlabel(\"\\nLabel\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.ylim([0, 100])\n",
    "plt.bar([labels[idx] for idx, i in enumerate(true_unique)], seg_pred_acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70668c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion matrix as table ready for export to csv\n",
    "df = pd.DataFrame(cm, columns=seg_labels, index=seg_labels)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0168ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "df.to_csv(\"D:/ConfusionMatrix/_Summary - Segmentation Error ComplexRotation Confusion Matrix.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eebf58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cd6f05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349abfed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc950f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e97235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4578d756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b97e076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67b0350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5afb141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04270c76",
   "metadata": {},
   "source": [
    "## Real-Time Inference with Threaded Solution\n",
    "When setting \"sliding_frames\" variable (used in ShowFrames class):\n",
    "- False: Uses frame pairs (RightCam, LeftCam)\n",
    "- True: Uses sliding frames (In -> RightCam -> LeftCam -> Out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeef961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from datetime import datetime\n",
    "from queue import Queue\n",
    "from time import sleep\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Decode for video\n",
    "def decode_one_hot_vid(img):\n",
    "    decoded = tf.math.argmax(img, axis=2)\n",
    "    return np.asarray(decoded, dtype='uint8')\n",
    "\n",
    "\n",
    "# Define class object for recording frame rate\n",
    "class FPS:\n",
    "    def __init__(self):\n",
    "        # store the start time, end time, and total number of frames\n",
    "        # that were examined between the start and end intervals\n",
    "        self._start = None\n",
    "        self._end = None\n",
    "        self._numFrames = 0\n",
    "\n",
    "    def start(self):\n",
    "        # start the timer\n",
    "        self._start = datetime.now()\n",
    "        return self\n",
    "\n",
    "    def stop(self):\n",
    "        # stop the timer\n",
    "        self._end = datetime.now()\n",
    "        \n",
    "    def now(self):\n",
    "        # record current time elapsed\n",
    "        return (datetime.now() - self._start).total_seconds()\n",
    "\n",
    "    def update(self, step):\n",
    "        # increment the total number of frames examined during the start and end intervals\n",
    "        self._numFrames += step\n",
    "\n",
    "    def elapsed(self):\n",
    "        # return the total number of seconds between the start and end interval\n",
    "        return (self._end - self._start).total_seconds()\n",
    "\n",
    "    def fps(self):\n",
    "        # compute the (approximate) frames per second\n",
    "        return self._numFrames / self.elapsed()\n",
    "    \n",
    "    \n",
    "class LoadFrames:\n",
    "    \"\"\"\n",
    "    Class that loads frames from a VideoCapture object.\n",
    "    Uses a dedicated thread.\n",
    "    \"\"\"\n",
    "    def __init__(self, source=0, queue_size=1024):\n",
    "        self.thread = Thread(target=self.fill_queue, args=())\n",
    "        self.thread.daemon = True\n",
    "        self.stream = cv2.VideoCapture(source)\n",
    "        self.Q = Queue(maxsize=queue_size)\n",
    "        self.grabbed = False\n",
    "        self.frame = None\n",
    "        self.count = 0\n",
    "    \n",
    "    def start(self):\n",
    "        self.thread.start()\n",
    "        return self\n",
    "\n",
    "    def fill_queue(self):\n",
    "        # Start infinite loop\n",
    "        while True:\n",
    "\n",
    "            # Fill Queue as long as room exists, else sleep to create room\n",
    "            if not self.Q.full():\n",
    "                # read the next frame from the file\n",
    "                (self.grabbed, self.frame) = self.stream.read()\n",
    "\n",
    "                # If no frames to grab assume end of file\n",
    "                if not self.grabbed:\n",
    "                    break\n",
    "\n",
    "                # LoadFrames quicker than ShowFrames, pre-process frames for model here\n",
    "                self.frame = cv2.cvtColor(self.frame, cv2.COLOR_BGR2RGB)\n",
    "                self.frame = cv2.resize(self.frame, (224, 224))\n",
    "                self.frame = self.frame.reshape(1, 224, 224, 3)\n",
    "                \n",
    "                # Add the frame to the queue\n",
    "                self.Q.put(self.frame)\n",
    "                self.count += 1\n",
    "\n",
    "            else:\n",
    "                sleep(0.1)\n",
    "\n",
    "        # Release stream and thread once finished grabbing frames\n",
    "        self.stream.release()\n",
    "        self.thread.join()\n",
    "\n",
    "    \n",
    "class ShowFrames:\n",
    "    \"\"\"\n",
    "    Class to retrieve frames from a pre-loaded queue and make DL predictions.\n",
    "    Uses a dedicated thread.\n",
    "    \"\"\"\n",
    "    def __init__(self, sliding_frames = False):\n",
    "        self.thread = Thread(target=self.show, args=())\n",
    "        self.thread.daemon = True\n",
    "        self.stopped = False\n",
    "        self.sliding_frames = sliding_frames\n",
    "        self.l_frame = None\n",
    "        self.l_frame_slide = None\n",
    "        self.r_frame = None\n",
    "        self.l_mask = None\n",
    "        self.depth = None\n",
    "\n",
    "        \n",
    "    def start(self):\n",
    "        self.thread.start()\n",
    "        return self\n",
    "\n",
    "    def show(self):\n",
    "\n",
    "        # Grab initial frames (right first, then left) - Known bug fix this waste of 2 frames\n",
    "        self.r_frame = get_frames.Q.get()\n",
    "        self.l_frame = get_frames.Q.get()\n",
    "        self.l_frame_slide = self.l_frame   # Creates a clean duplicate for sliding frames\n",
    "        \n",
    "        # Start loop to show frames\n",
    "        while not self.stopped:\n",
    "\n",
    "            # Grab sliding frames (right first, then left):\n",
    "            if self.sliding_frames:\n",
    "                self.r_frame = self.l_frame_slide\n",
    "                self.l_frame = get_frames.Q.get()\n",
    "                self.l_frame_slide = self.l_frame   # Creates a clean duplicate for sliding frames\n",
    "                fps.update(step=1)\n",
    "            else:\n",
    "                # Grab frame pairs\n",
    "                self.r_frame = get_frames.Q.get()\n",
    "                self.l_frame = get_frames.Q.get()\n",
    "                fps.update(step=1)\n",
    "\n",
    "            # Make depth prediction (if error, add depth = depth.reshape(224, 224, 1))\n",
    "            self.l_mask, _, self.depth = model.predict([self.l_frame/255., self.r_frame/255.])\n",
    "            # Returns shape (1, 224, 224, 256)\n",
    "            self.l_mask = self.l_mask.reshape((224, 224, 256))\n",
    "            self.l_mask = decode_one_hot_vid(self.l_mask)\n",
    "            self.depth = self.depth.reshape((224, 224, 256))\n",
    "            self.depth = decode_one_hot_vid(self.depth)\n",
    "            self.depth = cv2.applyColorMap(self.depth, cv2.COLORMAP_RAINBOW)\n",
    "            self.l_mask = cv2.applyColorMap(self.l_mask, cv2.COLORMAP_DEEPGREEN)\n",
    "\n",
    "            # Reshape input frame\n",
    "            self.l_frame = self.l_frame.reshape((224, 224, 3))\n",
    "            self.l_frame = cv2.cvtColor(self.l_frame, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Add frame rate and text\n",
    "            cv2.putText(self.l_frame, \"FPS: {:.2f}\".format(fps._numFrames / fps.now()),\n",
    "            (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "            cv2.putText(self.l_frame, \"Left Camera\", (10, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "            cv2.putText(self.l_mask, \"Segmentation\", (10, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "            cv2.putText(self.depth, \"Depth\", (10, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)\n",
    "\n",
    "            # Resize for clarity\n",
    "#             self.l_frame = cv2.resize(self.l_frame, (450, 450))\n",
    "#             self.l_mask = cv2.resize(self.l_mask, (450, 450))\n",
    "#             self.depth = cv2.resize(self.depth, (450, 450))\n",
    "            \n",
    "            # Show frames\n",
    "            concat = np.concatenate((self.l_frame, self.l_mask, self.depth), axis=1)\n",
    "            cv2.imshow(\"Left Frame - Left Mask - Depth (with colourmaps)\", concat)\n",
    "            \n",
    "            # Write frame for creating video\n",
    "            cv2.imwrite(f'D:/Video/complex/real_life/output_frames/{fps._numFrames}.png', concat)\n",
    "        \n",
    "            # Check if queue contains enough frames\n",
    "            if get_frames.Q.qsize() < 2:\n",
    "                if not self.sliding_frames:     # Frame pairs need minimum of 2 frames\n",
    "                    self.stopped = True\n",
    "                elif get_frames.Q.qsize == 0:   # Sliding frames can use 1 frame\n",
    "                    self.stopped = True\n",
    "            \n",
    "            # Escape video feed\n",
    "            key = cv2.waitKey(1) & 0xff\n",
    "            if key == ord('q'):\n",
    "                break \n",
    "\n",
    "        # Clean up\n",
    "        cv2.destroyAllWindows()\n",
    "        self.stop()\n",
    "\n",
    "    def stop(self):\n",
    "        self.stopped = True\n",
    "        fps.stop()\n",
    "        print(f\"Frames loaded:\\t{get_frames.count}\")\n",
    "        print(f\"Average FPS:\\t{int(round(fps.fps(), 2))}\")\n",
    "\n",
    "        \n",
    "# Define function to allow initialisation of threading\n",
    "def init_get_frames(source, Qsize):\n",
    "    t1 = LoadFrames(source, Qsize)\n",
    "    return t1\n",
    "\n",
    "\n",
    "# Define function to allow initialisation of threading\n",
    "def init_show_frames(sliding_frames):\n",
    "    t2 = ShowFrames(sliding_frames)\n",
    "    return t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab47f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load manually named saved model\n",
    "# model.load_weights('Models/_Summary_SemiComplexRotation_StereoDepthNetv2_20220215_180725.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37625e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define source and initialise objects.  Start timer.\n",
    "# source = 'D:/Video/videotest.mp4'\n",
    "source = 'D:/Video/complex/real_life/yesnaby_008.mp4'\n",
    "\n",
    "fps = FPS().start()\n",
    "\n",
    "# Initialise and start frame loading with max queue size variable\n",
    "get_frames = init_get_frames(source, 8).start()\n",
    "\n",
    "# Initialise frame showing thread with slding frames variable\n",
    "show_frames = init_show_frames(sliding_frames=False).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23863763",
   "metadata": {},
   "source": [
    "## Additional Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0904eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write frames to video\n",
    "import cv2\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "image_folder = 'D:/Video/complex/real_life/output_frames/'\n",
    "video_name = 'D:/Video/complex/real_life/yesnaby_008_COMPLEX.mp4'\n",
    "\n",
    "images = [img for img in os.listdir(image_folder) if img.endswith(\".png\")]\n",
    "images = natsorted(images)\n",
    "frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "height, width, layers = frame.shape\n",
    "\n",
    "video = cv2.VideoWriter(video_name, 0, 15, (width,height))\n",
    "\n",
    "for idx, image in enumerate(images):\n",
    "    video.write(cv2.imread(os.path.join(image_folder, images[idx])))\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc57e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59abe51c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb2ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc7511c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
